{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec for Description Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Use the description columns from our dataframe and create a Word2Vec model from it. We are trying to capture sentence similarity so after creating a Word2Vec Model what we can do is take every description from our description column and 'add' the words to get a vector for every description. Then using cosine similarity we can see if two sentences are similar. Note when we add we have to divide by the length of the sentence to take that into account. We want the words to impact the similarity not the length of the sentence per se.\n",
    "\n",
    "Outline\n",
    "* Create model on description columns -> grab all the words and apply it to Word2Vec\n",
    "* Apply the model on every single description of a real estate listing effectively adding every single word in the description to create a new vector that represents that description. Remember to divide by sentence length\n",
    "* Export those numpy arrays and to cosine similarity in feature building notebook \n",
    "* Reason for exporting the arrays was my kernel kept crashing, so wanted to save the output asap \n",
    "\n",
    "Note: We have to do Word2Vec on our training set then apply this model to our testing set. Word2Vec improves with more info so our results from this feature could be better with more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from collections import Counter\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['description_id1' , 'description_id2']\n",
    "df = pd.read_csv('cleaned_data.csv' , usecols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id1</th>\n",
       "      <th>description_id2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Strength, tradition and serenity around 10,000...</td>\n",
       "      <td>Magnificent Mallorquinian Mansion of XVII cent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Magnificent Mallorquinian Mansion of XVII cent...</td>\n",
       "      <td>Magnificent Mallorquinian Mansion of XVII cent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     description_id1  \\\n",
       "0  Strength, tradition and serenity around 10,000...   \n",
       "1  Magnificent Mallorquinian Mansion of XVII cent...   \n",
       "\n",
       "                                     description_id2  \n",
       "0  Magnificent Mallorquinian Mansion of XVII cent...  \n",
       "1  Magnificent Mallorquinian Mansion of XVII cent...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2) # Build Word2Vec of words on description columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string data for description_id1 column\n",
    "description_id1 = []\n",
    "for i in df['description_id1']:\n",
    "    description_id1.append(re.sub(r'\\W+', ' ', i ).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize description_id1\n",
    "description_id1 = [nltk.word_tokenize(sentence) for sentence in description_id1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords from description_id1\n",
    "for i in range(len(description_id1)):\n",
    "    description_id1[i] = [word for word in description_id1[i] if word not in stopwords_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string data for description_id2 column\n",
    "description_id2 = []\n",
    "for i in df['description_id2']:\n",
    "    description_id2.append(re.sub(r'\\W+', ' ', i ).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize description_id2\n",
    "description_id2 = [nltk.word_tokenize(sentence) for sentence in description_id2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords from description_id2\n",
    "for i in range(len(description_id2)):\n",
    "    description_id2[i] = [word for word in description_id2[i] if word not in stopwords_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tokenized columns\n",
    "description = description_id1 + description_id2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(description, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"TrainWord2vecDescription.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bread', 0.6565971374511719),\n",
       " ('bake', 0.6351786851882935),\n",
       " ('oven', 0.6210470795631409),\n",
       " ('drawer', 0.6147385835647583),\n",
       " ('woodfired', 0.5470974445343018),\n",
       " ('alfreso', 0.540163516998291),\n",
       " ('bullerjan', 0.5209804773330688),\n",
       " ('microwave', 0.5185470581054688),\n",
       " ('grill', 0.5059233903884888),\n",
       " ('freezer', 0.4947850704193115)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('pizza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22031"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model on Description_id1 & Description_id2 columns from training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = Word2Vec.load(\"TrainWord2vecDescription.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22031"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description_id1 Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to divide by len(vec) so sentence length doesn't mess things up. Instead we want to focus on the word similarity. \n",
    "# Few sentences are length 0 as in no description so to avoid dividing by 0 we'll just append a (100,1) vector of ones for simplicity\n",
    "description_1_vector_sums = []\n",
    "for i in range(len(description_id1)):\n",
    "    vec = []\n",
    "    for word in description_id1[i]:\n",
    "        vec.append(model.wv[word])\n",
    "    if len(vec) > 0:\n",
    "        description_1_vector_sums.append(sum(vec)/len(vec))\n",
    "    else:\n",
    "        description_1_vector_sums.append(np.ones(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1np = np.asarray(description_1_vector_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502689"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d1np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('d1np.npy' , d1np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description_id2 Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had to re-do the code below because my kernel crashed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['description_id1' , 'description_id2']\n",
    "df = pd.read_csv('cleaned_data.csv' , usecols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = Word2Vec.load(\"TrainWord2vecDescription.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string data for description_id2 column\n",
    "description_id2 = []\n",
    "for i in df['description_id2']:\n",
    "    description_id2.append(re.sub(r'\\W+', ' ', i ).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize description_id2\n",
    "description_id2 = [nltk.word_tokenize(sentence) for sentence in description_id2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords from description_id2\n",
    "for i in range(len(description_id2)):\n",
    "    description_id2[i] = [word for word in description_id2[i] if word not in stopwords_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to divide by len(vec) so sentence length doesn't mess things up. Instead we want to focus on the word similarity. \n",
    "# Few sentences are length 0 as in no description so to avoid dividing by 0 we'll just append a (100,1) vector of ones for simplicity\n",
    "description_2_vector_sums = []\n",
    "for i in range(len(description_id2)):\n",
    "    vec = []\n",
    "    for word in description_id2[i]:\n",
    "        vec.append(model.wv[word])\n",
    "    if len(vec) > 0:\n",
    "        description_2_vector_sums.append(sum(vec)/len(vec))\n",
    "    else:\n",
    "        description_2_vector_sums.append(np.ones(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2np = np.asarray(description_2_vector_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('d2np.npy',d2np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
