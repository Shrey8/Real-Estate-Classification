{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from collections import Counter\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Pipeline Set-Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab Numerical Data\n",
    "def numFeat(data):\n",
    "    cat_feats = data.dtypes[data.dtypes == 'object'].index.tolist()\n",
    "    num_feats = data.dtypes[~data.dtypes.index.isin(cat_feats)].index.tolist()\n",
    "    return data[num_feats]\n",
    "\n",
    "# Create above function into a FunctionTransformer\n",
    "keep_num = FunctionTransformer(numFeat)\n",
    "\n",
    "# Create Feature Transformer on select columns (only numerical in our case)\n",
    "class SelectColumnsTransformer():\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, data, **transform_params):\n",
    "        price_difference = np.abs(data['price_id1'] - data['price_id2'])\n",
    "        bedroom_difference = np.abs(data['bedrooms_id1'] - data['bedrooms_id2'])\n",
    "        bathroom_difference = np.abs(data['bathrooms_id1'] - data['bathrooms_id2'])\n",
    "        area_difference = np.abs(data['totalArea_id1'] - data['totalArea_id2'])\n",
    "        apartment_dummy_difference = np.abs(data['apartment_dummy_1'] - data['apartment_dummy_2'])\n",
    "        house_dummy_difference = np.abs(data['house_dummy_1'] - data['house_dummy_2'])\n",
    "        plot_dummy_difference = np.abs(data['plot_dummy_1'] - data['plot_dummy_2'])\n",
    "        investment_dummy_difference = np.abs(data['investment_dummy_1'] - data['investment_dummy_2'])\n",
    "        other_dummy_difference = np.abs(data['other_dummy_1'] - data['other_dummy_2'])\n",
    "        \n",
    "        features = pd.DataFrame()\n",
    "        features['price_difference'] = price_difference\n",
    "        features['bedroom_difference'] = bedroom_difference\n",
    "        features['bathroom_difference'] = bathroom_difference\n",
    "        features['area_difference'] = area_difference\n",
    "        features['apartment_dummy_difference'] = apartment_dummy_difference\n",
    "        features['house_dummy_difference'] = house_dummy_difference\n",
    "        features['plot_dummy_difference'] = plot_dummy_difference\n",
    "        features['investment_dummy_difference'] = investment_dummy_difference\n",
    "        features['other_dummy_difference'] = other_dummy_difference\n",
    "        \n",
    "        #data = features\n",
    "\n",
    "        return features       \n",
    "    \n",
    "    \n",
    "    def fit(self, data, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Pipeline Set-Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab String Data\n",
    "def catFeat(data):\n",
    "    cat_feats = data.dtypes[data.dtypes == 'object'].index.tolist()\n",
    "    #num_feats = data.dtypes[~data.dtypes.index.isin(cat_feats)].index.tolist()\n",
    "    return data[cat_feats]\n",
    "\n",
    "# Create above function into a FunctionTransformer\n",
    "keep_cat = FunctionTransformer(catFeat)\n",
    "\n",
    "class Word2VecTransformer():\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        self.Word2VecTitle = Word2Vec.load(\"/Users/Shrey/LHL_Notes/Final_Project/casaData/TestWord2vecTitle.model\")\n",
    "        self.Word2VecDescription = Word2Vec.load(\"/Users/Shrey/LHL_Notes/Final_Project/casaData/TestWord2vecDescription.model\")\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        \n",
    "        #Title Columns\n",
    "    \n",
    "        title_1 = []\n",
    "        for i in df['title_id1']:\n",
    "            title_1.append(re.sub(r'\\W+', ' ', i.lower()))\n",
    "            \n",
    "        # Tokenize words in title_1\n",
    "        title_1 = [nltk.word_tokenize(sentence) for sentence in title_1]\n",
    "        \n",
    "        # Remove stopwords\n",
    "        for i in range(len(title_1)):\n",
    "            title_1[i] = [word for word in title_1[i] if word not in stopwords_dict]\n",
    "        \n",
    "        # Clean string data for title_id2 column\n",
    "        title_2 = []\n",
    "        for i in df['title_id2']:\n",
    "            title_2.append(re.sub(r'\\W+', ' ', i.lower()))\n",
    "            \n",
    "        # Tokenize words in title_2\n",
    "        title_2 = [nltk.word_tokenize(sentence) for sentence in title_2]\n",
    "        \n",
    "        # Remove stopwords\n",
    "        for i in range(len(title_2)):\n",
    "            title_2[i] = [word for word in title_2[i] if word not in stopwords_dict]\n",
    "            \n",
    "        title = title_1 + title_2\n",
    "        \n",
    "        self.Word2VecTitle.build_vocab(title, update=True)\n",
    "        \n",
    "        self.Word2VecTitle.train(title, total_examples=self.Word2VecTitle.corpus_count ,epochs=1)\n",
    "        \n",
    "        title_1_vector_sums = []\n",
    "        for i in range(len(title_1)):\n",
    "            vec = []\n",
    "            for word in title_1[i]:\n",
    "                vec.append(self.Word2VecTitle.wv[word])\n",
    "            if len(vec) > 0:\n",
    "                title_1_vector_sums.append(sum(vec)/len(vec))\n",
    "            else:\n",
    "                title_1_vector_sums.append(sum(vec)/(len(vec)+1))\n",
    "            \n",
    "        title_2_vector_sums = []\n",
    "        for i in range(len(title_2)):\n",
    "            vec = []\n",
    "            for word in title_2[i]:\n",
    "                vec.append(self.Word2VecTitle.wv[word])\n",
    "            if len(vec) > 0:\n",
    "                title_2_vector_sums.append(sum(vec)/len(vec))\n",
    "            else:\n",
    "                title_2_vector_sums.append(sum(vec)/(len(vec)+1))\n",
    "                \n",
    "        test_t1np = np.asarray(title_1_vector_sums)\n",
    "        \n",
    "        test_t2np = np.asarray(title_2_vector_sums)\n",
    "        \n",
    "        \n",
    "        # Description Columns \n",
    "        description_id1 = []\n",
    "        for i in df['description_id1']:\n",
    "            description_id1.append(re.sub(r'\\W+', ' ', i ).lower())\n",
    "            \n",
    "        # Tokenize description_id1\n",
    "        description_id1 = [nltk.word_tokenize(sentence) for sentence in description_id1]\n",
    "        \n",
    "        # Remove Stopwords from description_id1\n",
    "        for i in range(len(description_id1)):\n",
    "            description_id1[i] = [word for word in description_id1[i] if word not in stopwords_dict]\n",
    "        \n",
    "        # Clean string data for description_id2 column\n",
    "        description_id2 = []\n",
    "        for i in df['description_id2']:\n",
    "            description_id2.append(re.sub(r'\\W+', ' ', i ).lower())\n",
    "            \n",
    "        # Tokenize description_id2\n",
    "        description_id2 = [nltk.word_tokenize(sentence) for sentence in description_id2]\n",
    "         \n",
    "        # Remove Stopwords from description_id2\n",
    "        for i in range(len(description_id2)):\n",
    "            description_id2[i] = [word for word in description_id2[i] if word not in stopwords_dict]\n",
    "        \n",
    "        # Combine tokenized columns\n",
    "        description = description_id1 + description_id2\n",
    "        \n",
    "        self.Word2VecDescription.build_vocab(description, update=True)\n",
    "        \n",
    "        self.Word2VecDescription.train(description, total_examples=self.Word2VecDescription.corpus_count ,epochs=1)\n",
    "        \n",
    "        description_1_vector_sums = []\n",
    "        for i in range(len(description_id1)):\n",
    "            vec = []\n",
    "            for word in description_id1[i]:\n",
    "                vec.append(self.Word2VecDescription.wv[word])\n",
    "            if len(vec) > 0:\n",
    "                description_1_vector_sums.append(sum(vec)/len(vec))\n",
    "            else:\n",
    "                description_1_vector_sums.append(np.ones(100))\n",
    "                \n",
    "                \n",
    "        description_2_vector_sums = []\n",
    "        for i in range(len(description_id2)):\n",
    "            vec = []\n",
    "            for word in description_id2[i]:\n",
    "                vec.append(self.Word2VecDescription.wv[word])\n",
    "            if len(vec) > 0:\n",
    "                description_2_vector_sums.append(sum(vec)/len(vec))\n",
    "            else:\n",
    "                description_2_vector_sums.append(np.ones(100))\n",
    "                \n",
    "        test_d1np = np.asarray(description_1_vector_sums)\n",
    "        \n",
    "        test_d2np = np.asarray(description_2_vector_sums)    \n",
    "        \n",
    "        ## Title Cosine Similarities \n",
    "\n",
    "        #Calculate description cosine similarity\n",
    "        test_description_cos_similarity = []\n",
    "        for i in range(len(test_d1np)):\n",
    "                test_description_cos_similarity.append(np.dot(test_d1np[i],test_d2np[i])/(norm(test_d1np[i])*norm(test_d2np[i])))\n",
    "                \n",
    "        # Calculate test cosine similarity\n",
    "        test_title_cos_similarity = []\n",
    "        for i in range(len(test_t1np)):\n",
    "                test_title_cos_similarity.append(np.dot(test_t1np[i],test_t2np[i])/(norm(test_t1np[i])*norm(test_t2np[i])))\n",
    "                \n",
    "                \n",
    "        features = pd.DataFrame()\n",
    "        features['description_cos_similarity'] = test_description_cos_similarity\n",
    "        features['title_cos_similarity'] = test_title_cos_similarity\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def fit(self, data, y=None, **fit_params):\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier(l2_regularization=0.0, learning_rate=0.1,\n",
       "                               loss='binary_crossentropy', max_bins=255,\n",
       "                               max_depth=None, max_iter=2000, max_leaf_nodes=31,\n",
       "                               min_samples_leaf=20, n_iter_no_change=None,\n",
       "                               random_state=None, scoring=None, tol=1e-07,\n",
       "                               validation_fraction=0.1, verbose=0,\n",
       "                               warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = pickle.load(open('hist_gradient_boosting_finalized_model.sav', 'rb'))\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    (\"num_feats\", keep_num),\n",
    "    (\"new_features\" , SelectColumnsTransformer())])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"cat_feats\", keep_cat),\n",
    "    (\"word_2_vec\", Word2VecTransformer())])\n",
    "\n",
    "all_features = FeatureUnion([\n",
    "    ('numeric_features', num_pipeline),\n",
    "    ('categorical_features', cat_pipeline)])\n",
    "\n",
    "main_pipeline = Pipeline([\n",
    "    ('all_features', all_features),\n",
    "    ('modeling', loaded_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
