{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec for Title Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Exact same outline as Word2Vec_Description.ipynb except this time we will create a new Word2Vec model to learn on the title columns\n",
    "\n",
    "Outline:\n",
    "* Similar to Word2Vec_Description.ipynb train on the title columns from original dataset. Then map it to our cleaned dataset (EDA_and_Data_Cleaning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from collections import Counter\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['title_id1' , 'title_id2']\n",
    "df = pd.read_csv('cleaned_data.csv' , usecols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_id1</th>\n",
       "      <th>title_id2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exquisite manor house amidst the Tramontana va...</td>\n",
       "      <td>Finca in Puigpunyent (Objektnummer KSV00142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Magnificent Mallorquinian Mansion of XVII cent...</td>\n",
       "      <td>Finca in Puigpunyent (Objektnummer KSV00142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Magnificent Mallorquinian Mansion of XVII cent...</td>\n",
       "      <td>Finca in Puigpunyent (Objektnummer KSV00142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mallorquinian Mansion of XVII century on the m...</td>\n",
       "      <td>Finca in Puigpunyent (Objektnummer KSV00142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unique Majorcan Rural Estate in Puigpunyent</td>\n",
       "      <td>Finca in Puigpunyent (Objektnummer KSV00142)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title_id1  \\\n",
       "0  Exquisite manor house amidst the Tramontana va...   \n",
       "1  Magnificent Mallorquinian Mansion of XVII cent...   \n",
       "2  Magnificent Mallorquinian Mansion of XVII cent...   \n",
       "3  Mallorquinian Mansion of XVII century on the m...   \n",
       "4        Unique Majorcan Rural Estate in Puigpunyent   \n",
       "\n",
       "                                      title_id2  \n",
       "0  Finca in Puigpunyent (Objektnummer KSV00142)  \n",
       "1  Finca in Puigpunyent (Objektnummer KSV00142)  \n",
       "2  Finca in Puigpunyent (Objektnummer KSV00142)  \n",
       "3  Finca in Puigpunyent (Objektnummer KSV00142)  \n",
       "4  Finca in Puigpunyent (Objektnummer KSV00142)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string data for title_id1 column\n",
    "title_1 = []\n",
    "for i in df['title_id1']:\n",
    "    title_1.append(re.sub(r'\\W+', ' ', i.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words in title_1\n",
    "title_1 = [nltk.word_tokenize(sentence) for sentence in title_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "for i in range(len(title_1)):\n",
    "    title_1[i] = [word for word in title_1[i] if word not in stopwords_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string data for title_id2 column\n",
    "title_2 = []\n",
    "for i in df['title_id2']:\n",
    "    title_2.append(re.sub(r'\\W+', ' ', i.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words in title_2\n",
    "title_2 = [nltk.word_tokenize(sentence) for sentence in title_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "for i in range(len(title_2)):\n",
    "    title_2[i] = [word for word in title_2[i] if word not in stopwords_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = title_1 + title_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005378"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(title, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"TrainWord2vecTitle.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model on Title_id1 & Title_id2 columns from cleaned data set (EDA_and_Data_Cleaning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title_id1 Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to divide by len(vec) so sentence length doesn't mess things up. Instead we want to focus on the word similarity. \n",
    "# Some sentences are length 0 as in no description so to avoid dividing by 0 just add a 1 and the resulting sum will be 0\n",
    "title_1_vector_sums = []\n",
    "for i in range(len(title_1)):\n",
    "    vec = []\n",
    "    for word in title_1[i]:\n",
    "        vec.append(model.wv[word])\n",
    "    if len(vec) > 0:\n",
    "        title_1_vector_sums.append(sum(vec)/len(vec))\n",
    "    else:\n",
    "        title_1_vector_sums.append(sum(vec)/(len(vec)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1np = np.asarray(title_1_vector_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502689"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t1np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('t1np.npy' , t1np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title_id2 Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to divide by len(vec) so sentence length doesn't mess things up. Instead we want to focus on the word similarity. \n",
    "# Some sentences are length 0 as in no description so to avoid dividing by 0 just add a 1 and the resulting sum will be 0\n",
    "title_2_vector_sums = []\n",
    "for i in range(len(title_2)):\n",
    "    vec = []\n",
    "    for word in title_2[i]:\n",
    "        vec.append(model.wv[word])\n",
    "    if len(vec) > 0:\n",
    "        title_2_vector_sums.append(sum(vec)/len(vec))\n",
    "    else:\n",
    "        title_2_vector_sums.append(sum(vec)/(len(vec)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2np = np.asarray(title_2_vector_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502689"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t2np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('t2np.npy' , t2np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
