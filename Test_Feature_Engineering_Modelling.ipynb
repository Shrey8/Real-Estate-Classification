{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from collections import Counter\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Word2Vec on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model_description = Word2Vec.load(\"TrainWord2vecDescription.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string data for description_id1 column\n",
    "description_id1 = []\n",
    "for i in df['description_id1']:\n",
    "    description_id1.append(re.sub(r'\\W+', ' ', i ).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize description_id1\n",
    "description_id1 = [nltk.word_tokenize(sentence) for sentence in description_id1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords from description_id1\n",
    "for i in range(len(description_id1)):\n",
    "    description_id1[i] = [word for word in description_id1[i] if word not in stopwords_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string data for description_id2 column\n",
    "description_id2 = []\n",
    "for i in df['description_id2']:\n",
    "    description_id2.append(re.sub(r'\\W+', ' ', i ).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize description_id2\n",
    "description_id2 = [nltk.word_tokenize(sentence) for sentence in description_id2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords from description_id2\n",
    "for i in range(len(description_id2)):\n",
    "    description_id2[i] = [word for word in description_id2[i] if word not in stopwords_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tokenized columns\n",
    "description = description_id1 + description_id2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_description.build_vocab(description, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_description.train(description, total_examples=model_description.corpus_count ,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "model_description.save(\"TestWord2vecDescription.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description_id1 column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to divide by len(vec) so sentence length doesn't mess things up. Instead we want to focus on the word similarity. \n",
    "# Few sentences are length 0 as in no description so to avoid dividing by 0 we'll just append a (100,1) vector of ones for simplicity\n",
    "description_1_vector_sums = []\n",
    "for i in range(len(description_id1)):\n",
    "    vec = []\n",
    "    for word in description_id1[i]:\n",
    "        vec.append(model_description.wv[word])\n",
    "    if len(vec) > 0:\n",
    "        description_1_vector_sums.append(sum(vec)/len(vec))\n",
    "    else:\n",
    "        description_1_vector_sums.append(np.ones(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d1np = np.asarray(description_1_vector_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215438"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_d1np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_d1np.npy' , test_d1np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description_id2 column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to divide by len(vec) so sentence length doesn't mess things up. Instead we want to focus on the word similarity. \n",
    "# Few sentences are length 0 as in no description so to avoid dividing by 0 we'll just append a (100,1) vector of ones for simplicity\n",
    "description_2_vector_sums = []\n",
    "for i in range(len(description_id2)):\n",
    "    vec = []\n",
    "    for word in description_id2[i]:\n",
    "        vec.append(model_description.wv[word])\n",
    "    if len(vec) > 0:\n",
    "        description_2_vector_sums.append(sum(vec)/len(vec))\n",
    "    else:\n",
    "        description_2_vector_sums.append(np.ones(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d2np = np.asarray(description_2_vector_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215438"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_d2np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_d2np.npy' , test_d2np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model_title = Word2Vec.load(\"TrainWord2vecTitle.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string data for title_id1 column\n",
    "title_1 = []\n",
    "for i in df['title_id1']:\n",
    "    title_1.append(re.sub(r'\\W+', ' ', i.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words in title_1\n",
    "title_1 = [nltk.word_tokenize(sentence) for sentence in title_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "for i in range(len(title_1)):\n",
    "    title_1[i] = [word for word in title_1[i] if word not in stopwords_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string data for title_id2 column\n",
    "title_2 = []\n",
    "for i in df['title_id2']:\n",
    "    title_2.append(re.sub(r'\\W+', ' ', i.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words in title_2\n",
    "title_2 = [nltk.word_tokenize(sentence) for sentence in title_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "for i in range(len(title_2)):\n",
    "    title_2[i] = [word for word in title_2[i] if word not in stopwords_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = title_1 + title_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430876"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_title.build_vocab(title, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1936111, 2900671)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_title.train(title, total_examples=model_title.corpus_count ,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "model_title.save(\"TestWord2vecTitle.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply model on Title_id1 column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to divide by len(vec) so sentence length doesn't mess things up. Instead we want to focus on the word similarity. \n",
    "# If sentences are of length 0 as in no description, to avoid dividing by 0 I'll just add a 1 and the resulting sum will be 0\n",
    "title_1_vector_sums = []\n",
    "for i in range(len(title_1)):\n",
    "    vec = []\n",
    "    for word in title_1[i]:\n",
    "        vec.append(model_title.wv[word])\n",
    "    if len(vec) > 0:\n",
    "        title_1_vector_sums.append(sum(vec)/len(vec))\n",
    "    else:\n",
    "        title_1_vector_sums.append(sum(vec)/(len(vec)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_t1np = np.asarray(title_1_vector_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215438"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_t1np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_t1np.npy' , test_t1np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply model on Title_id2 column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to divide by len(vec) so sentence length doesn't mess things up. Instead we want to focus on the word similarity. \n",
    "# Some sentences are length 0 as in no description so to avoid dividing by 0 just add a 1 and the resulting sum will be 0\n",
    "title_2_vector_sums = []\n",
    "for i in range(len(title_2)):\n",
    "    vec = []\n",
    "    for word in title_2[i]:\n",
    "        vec.append(model_title.wv[word])\n",
    "    if len(vec) > 0:\n",
    "        title_2_vector_sums.append(sum(vec)/len(vec))\n",
    "    else:\n",
    "        title_2_vector_sums.append(sum(vec)/(len(vec)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_t2np = np.asarray(title_2_vector_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215438"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_t2np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_t2np.npy' , test_t2np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price_id1', 'bedrooms_id1', 'bathrooms_id1', 'totalArea_id1',\n",
       "       'price_id2', 'bedrooms_id2', 'bathrooms_id2', 'totalArea_id2',\n",
       "       'apartment_dummy_1', 'house_dummy_1', 'plot_dummy_1',\n",
       "       'investment_dummy_1', 'other_dummy_1', 'apartment_dummy_2',\n",
       "       'house_dummy_2', 'plot_dummy_2', 'investment_dummy_2', 'other_dummy_2',\n",
       "       'target', 'title_id1', 'title_id2', 'description_id1',\n",
       "       'description_id2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Features (differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_difference = np.abs(df['price_id1'] - df['price_id2'])\n",
    "bedroom_difference = np.abs(df['bedrooms_id1'] - df['bedrooms_id2'])\n",
    "bathroom_difference = np.abs(df['bathrooms_id1'] - df['bathrooms_id2'])\n",
    "area_difference = np.abs(df['totalArea_id1'] - df['totalArea_id2'])\n",
    "apartment_dummy_difference = np.abs(df['apartment_dummy_1'] - df['apartment_dummy_2'])\n",
    "house_dummy_difference = np.abs(df['house_dummy_1'] - df['house_dummy_2'])\n",
    "plot_dummy_difference = np.abs(df['plot_dummy_1'] - df['plot_dummy_2'])\n",
    "investment_dummy_difference = np.abs(df['investment_dummy_1'] - df['investment_dummy_2'])\n",
    "other_dummy_difference = np.abs(df['other_dummy_1'] - df['other_dummy_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description columns\n",
    "\n",
    "\n",
    "test_d1np = np.load('test_d1np.npy')\n",
    "test_d2np = np.load('test_d2np.npy')\n",
    "\n",
    "#Calculate cosine similarity\n",
    "test_description_cos_similarity = []\n",
    "for i in range(len(test_d1np)):\n",
    "        test_description_cos_similarity.append(np.dot(test_d1np[i],test_d2np[i])/(norm(test_d1np[i])*norm(test_d2np[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title columns\n",
    "\n",
    "test_t1np = np.load('test_t1np.npy')\n",
    "test_t2np = np.load('test_t2np.npy')\n",
    "\n",
    "\n",
    "# Calculate cosine similarity\n",
    "test_title_cos_similarity = []\n",
    "for i in range(len(test_t1np)):\n",
    "        test_title_cos_similarity.append(np.dot(test_t1np[i],test_t2np[i])/(norm(test_t1np[i])*norm(test_t2np[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['price_difference'] = price_difference\n",
    "features['bedroom_difference'] = bedroom_difference\n",
    "features['bathroom_difference'] = bathroom_difference\n",
    "features['area_difference'] = area_difference\n",
    "features['apartment_dummy_difference'] = apartment_dummy_difference\n",
    "features['house_dummy_difference'] = house_dummy_difference\n",
    "features['plot_dummy_difference'] = plot_dummy_difference\n",
    "features['investment_dummy_difference'] = investment_dummy_difference\n",
    "features['other_dummy_difference'] = other_dummy_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['description_cos_similarity'] = test_description_cos_similarity\n",
    "features['title_cos_similarity'] = test_title_cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['target']  = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215438, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price_difference               float64\n",
       "bedroom_difference             float64\n",
       "bathroom_difference            float64\n",
       "area_difference                float64\n",
       "apartment_dummy_difference     float64\n",
       "house_dummy_difference         float64\n",
       "plot_dummy_difference          float64\n",
       "investment_dummy_difference    float64\n",
       "other_dummy_difference         float64\n",
       "description_cos_similarity     float64\n",
       "title_cos_similarity           float64\n",
       "target                           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.iloc[:,:-1].values\n",
    "y = features['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_difference</th>\n",
       "      <th>bedroom_difference</th>\n",
       "      <th>bathroom_difference</th>\n",
       "      <th>area_difference</th>\n",
       "      <th>apartment_dummy_difference</th>\n",
       "      <th>house_dummy_difference</th>\n",
       "      <th>plot_dummy_difference</th>\n",
       "      <th>investment_dummy_difference</th>\n",
       "      <th>other_dummy_difference</th>\n",
       "      <th>description_cos_similarity</th>\n",
       "      <th>title_cos_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.723658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469201</td>\n",
       "      <td>0.668713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.745617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.706801</td>\n",
       "      <td>0.827872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.926298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.401719</td>\n",
       "      <td>0.668713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.548720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550554</td>\n",
       "      <td>0.686827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.603020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.548579</td>\n",
       "      <td>0.787015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215433</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.947415</td>\n",
       "      <td>0.413101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215434</th>\n",
       "      <td>82000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.651286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097064</td>\n",
       "      <td>0.129680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215435</th>\n",
       "      <td>85000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.754105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.106588</td>\n",
       "      <td>0.362659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215436</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491197</td>\n",
       "      <td>0.716594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215437</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039110</td>\n",
       "      <td>0.298884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215438 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        price_difference  bedroom_difference  bathroom_difference  \\\n",
       "0                26400.0                 0.0                  0.0   \n",
       "1                36400.0                 0.0                  0.0   \n",
       "2                10000.0                 0.0                  0.0   \n",
       "3                10000.0                 0.0                  0.0   \n",
       "4                10000.0                 0.0                  0.0   \n",
       "...                  ...                 ...                  ...   \n",
       "215433          100000.0                 0.0                  0.0   \n",
       "215434           82000.0                 0.0                  0.0   \n",
       "215435           85000.0                 0.0                  0.0   \n",
       "215436           20000.0                 0.0                  0.0   \n",
       "215437           60000.0                 1.0                  0.0   \n",
       "\n",
       "        area_difference  apartment_dummy_difference  house_dummy_difference  \\\n",
       "0             36.723658                         0.0                     0.0   \n",
       "1              5.745617                         0.0                     0.0   \n",
       "2             19.926298                         0.0                     0.0   \n",
       "3             29.548720                         0.0                     0.0   \n",
       "4              1.603020                         0.0                     0.0   \n",
       "...                 ...                         ...                     ...   \n",
       "215433       107.000000                         0.0                     0.0   \n",
       "215434        73.651286                         0.0                     0.0   \n",
       "215435         0.754105                         0.0                     0.0   \n",
       "215436        10.000000                         0.0                     0.0   \n",
       "215437        97.000000                         0.0                     0.0   \n",
       "\n",
       "        plot_dummy_difference  investment_dummy_difference  \\\n",
       "0                         0.0                          0.0   \n",
       "1                         0.0                          0.0   \n",
       "2                         0.0                          0.0   \n",
       "3                         0.0                          0.0   \n",
       "4                         0.0                          0.0   \n",
       "...                       ...                          ...   \n",
       "215433                    0.0                          0.0   \n",
       "215434                    0.0                          0.0   \n",
       "215435                    0.0                          0.0   \n",
       "215436                    0.0                          0.0   \n",
       "215437                    0.0                          0.0   \n",
       "\n",
       "        other_dummy_difference  description_cos_similarity  \\\n",
       "0                          0.0                    0.469201   \n",
       "1                          0.0                    0.706801   \n",
       "2                          0.0                    0.401719   \n",
       "3                          0.0                    0.550554   \n",
       "4                          0.0                    0.548579   \n",
       "...                        ...                         ...   \n",
       "215433                     0.0                    0.947415   \n",
       "215434                     0.0                    0.097064   \n",
       "215435                     0.0                   -0.106588   \n",
       "215436                     0.0                    0.491197   \n",
       "215437                     0.0                    0.039110   \n",
       "\n",
       "        title_cos_similarity  \n",
       "0                   0.668713  \n",
       "1                   0.827872  \n",
       "2                   0.668713  \n",
       "3                   0.686827  \n",
       "4                   0.787015  \n",
       "...                      ...  \n",
       "215433              0.413101  \n",
       "215434              0.129680  \n",
       "215435              0.362659  \n",
       "215436              0.716594  \n",
       "215437              0.298884  \n",
       "\n",
       "[215438 rows x 11 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply HistGradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " # explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "# now you can import normally from ensemble\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('hist_gradient_boosting_finalized_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier(l2_regularization=0.0, learning_rate=0.1,\n",
       "                               loss='binary_crossentropy', max_bins=255,\n",
       "                               max_depth=None, max_iter=2000, max_leaf_nodes=31,\n",
       "                               min_samples_leaf=20, n_iter_no_change=None,\n",
       "                               random_state=None, scoring=None, tol=1e-07,\n",
       "                               validation_fraction=0.1, verbose=0,\n",
       "                               warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of correct cases predicted is 81.15513512008096\n"
     ]
    }
   ],
   "source": [
    "c = confusion_matrix(y,y_pred)\n",
    "correct_cases = ((c[0][0] + c[1][1])/ (c[0][0] + c[1][1] + c[1][0] + c[0][1]))*100\n",
    "print(\"% of correct cases predicted is \" + str(correct_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
